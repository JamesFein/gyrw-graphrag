#!/usr/bin/env python3
"""
GraphRAG Parquetæ–‡ä»¶ç»“æ„åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆ†æGraphRAGç”Ÿæˆçš„æ‰€æœ‰parquetæ–‡ä»¶çš„ç»“æ„ï¼Œ
åŒ…æ‹¬å­—æ®µåç§°ã€æ•°æ®ç±»å‹ã€æ ·æœ¬æ•°æ®ç­‰ä¿¡æ¯ã€‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-06-27
"""

import pandas as pd
import os
import json
from pathlib import Path
from typing import Dict, List, Any
import pyarrow.parquet as pq


def analyze_parquet_file(file_path: str) -> Dict[str, Any]:
    """
    åˆ†æå•ä¸ªparquetæ–‡ä»¶çš„ç»“æ„
    
    Args:
        file_path: parquetæ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«æ–‡ä»¶åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # è¯»å–parquetæ–‡ä»¶
        df = pd.read_parquet(file_path)
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        file_info = {
            "file_name": os.path.basename(file_path),
            "file_path": file_path,
            "file_size_mb": round(os.path.getsize(file_path) / (1024 * 1024), 2),
            "row_count": len(df),
            "column_count": len(df.columns),
            "columns": {}
        }
        
        # åˆ†ææ¯ä¸ªåˆ—
        for col in df.columns:
            col_info = {
                "data_type": str(df[col].dtype),
                "null_count": df[col].isnull().sum(),
                "null_percentage": round((df[col].isnull().sum() / len(df)) * 100, 2),
                "sample_values": []
            }

            # å®‰å…¨åœ°è®¡ç®—å”¯ä¸€å€¼æ•°é‡
            try:
                col_info["unique_count"] = df[col].nunique()
            except Exception:
                col_info["unique_count"] = "Cannot calculate (complex data type)"
            
            # è·å–æ ·æœ¬å€¼ï¼ˆéç©ºå€¼ï¼‰
            try:
                non_null_values = df[col].dropna()
                if len(non_null_values) > 0:
                    sample_size = min(5, len(non_null_values))
                    # å¤„ç†å¯èƒ½çš„æ•°ç»„ç±»å‹æ•°æ®
                    sample_values = []
                    for val in non_null_values.head(sample_size):
                        if isinstance(val, (list, tuple, set)):
                            sample_values.append(f"[Array with {len(val)} items]")
                        elif hasattr(val, '__array__'):  # numpyæ•°ç»„
                            sample_values.append(f"[Array with {len(val)} items]")
                        else:
                            sample_values.append(str(val)[:100])  # é™åˆ¶å­—ç¬¦ä¸²é•¿åº¦
                    col_info["sample_values"] = sample_values
            except Exception as e:
                col_info["sample_values"] = [f"Error getting samples: {str(e)}"]
            
            # å¯¹äºæ•°å€¼ç±»å‹ï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                col_info["statistics"] = {
                    "min": df[col].min(),
                    "max": df[col].max(),
                    "mean": round(df[col].mean(), 4) if not df[col].isnull().all() else None,
                    "std": round(df[col].std(), 4) if not df[col].isnull().all() else None
                }
            
            file_info["columns"][col] = col_info
        
        return file_info
        
    except Exception as e:
        return {
            "file_name": os.path.basename(file_path),
            "file_path": file_path,
            "error": str(e)
        }


def find_parquet_files(directory: str) -> List[str]:
    """
    åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
    
    Args:
        directory: æœç´¢ç›®å½•
        
    Returns:
        parquetæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    parquet_files = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.parquet'):
                parquet_files.append(os.path.join(root, file))
    return parquet_files


def generate_analysis_report(analysis_results: List[Dict[str, Any]]) -> str:
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    
    Args:
        analysis_results: åˆ†æç»“æœåˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Šå­—ç¬¦ä¸²
    """
    report = []
    report.append("=" * 80)
    report.append("GraphRAG Parquetæ–‡ä»¶ç»“æ„åˆ†ææŠ¥å‘Š")
    report.append("=" * 80)
    report.append(f"åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"åˆ†ææ–‡ä»¶æ•°é‡: {len(analysis_results)}")
    report.append("")
    
    # æ€»è§ˆ
    total_size = sum(result.get("file_size_mb", 0) for result in analysis_results if "error" not in result)
    total_rows = sum(result.get("row_count", 0) for result in analysis_results if "error" not in result)
    
    report.append("ğŸ“Š æ€»è§ˆç»Ÿè®¡")
    report.append("-" * 40)
    report.append(f"æ€»æ–‡ä»¶å¤§å°: {total_size:.2f} MB")
    report.append(f"æ€»è®°å½•æ•°: {total_rows:,}")
    report.append("")
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªæ–‡ä»¶
    for result in analysis_results:
        if "error" in result:
            report.append(f"âŒ é”™è¯¯æ–‡ä»¶: {result['file_name']}")
            report.append(f"   é”™è¯¯ä¿¡æ¯: {result['error']}")
            report.append("")
            continue
            
        report.append(f"ğŸ“ æ–‡ä»¶: {result['file_name']}")
        report.append("-" * 60)
        report.append(f"è·¯å¾„: {result['file_path']}")
        report.append(f"å¤§å°: {result['file_size_mb']} MB")
        report.append(f"è¡Œæ•°: {result['row_count']:,}")
        report.append(f"åˆ—æ•°: {result['column_count']}")
        report.append("")
        
        # åˆ—ä¿¡æ¯
        report.append("ğŸ“‹ åˆ—ä¿¡æ¯:")
        for col_name, col_info in result["columns"].items():
            report.append(f"  â€¢ {col_name}")
            report.append(f"    ç±»å‹: {col_info['data_type']}")
            report.append(f"    ç©ºå€¼: {col_info['null_count']} ({col_info['null_percentage']}%)")
            report.append(f"    å”¯ä¸€å€¼: {col_info['unique_count']}")
            
            if col_info["sample_values"]:
                sample_str = ", ".join(str(v)[:50] for v in col_info["sample_values"])
                report.append(f"    æ ·æœ¬: {sample_str}")
            
            if "statistics" in col_info and col_info["statistics"]:
                stats = col_info["statistics"]
                report.append(f"    ç»Ÿè®¡: min={stats['min']}, max={stats['max']}, mean={stats['mean']}")
            
            report.append("")
        
        report.append("")
    
    return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹åˆ†æGraphRAG Parquetæ–‡ä»¶ç»“æ„...")
    
    # æŸ¥æ‰¾outputç›®å½•ä¸­çš„æ‰€æœ‰parquetæ–‡ä»¶
    output_dir = "output"
    if not os.path.exists(output_dir):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å‡ºç›®å½• '{output_dir}'")
        return
    
    parquet_files = find_parquet_files(output_dir)
    
    if not parquet_files:
        print(f"âŒ åœ¨ç›®å½• '{output_dir}' ä¸­æœªæ‰¾åˆ°parquetæ–‡ä»¶")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶:")
    for file in parquet_files:
        print(f"  - {file}")
    print("")
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    analysis_results = []
    for file_path in parquet_files:
        print(f"ğŸ” åˆ†ææ–‡ä»¶: {os.path.basename(file_path)}")
        result = analyze_parquet_file(file_path)
        analysis_results.append(result)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_analysis_report(analysis_results)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = "parquet_analysis_report.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
    json_file = "parquet_analysis_data.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
    
    print("âœ… åˆ†æå®Œæˆ!")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
    print("")
    print("ğŸ“‹ åˆ†ææ‘˜è¦:")
    print("-" * 40)
    
    # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
    for result in analysis_results:
        if "error" not in result:
            print(f"{result['file_name']}: {result['row_count']:,} è¡Œ, {result['column_count']} åˆ—")
            if result['file_name'] in ['relationships.parquet', 'relationships_with_types.parquet']:
                print(f"  å…³ç³»æ–‡ä»¶åˆ—å: {list(result['columns'].keys())}")
    
    print("")
    print("ğŸ¯ é‡ç‚¹å…³æ³¨:")
    print("- entities.parquet: å®ä½“æ•°æ®ï¼ˆç”¨äºç”ŸæˆèŠ‚ç‚¹ï¼‰")
    print("- relationships.parquet: å…³ç³»æ•°æ®ï¼ˆç”¨äºç”Ÿæˆè¾¹ï¼‰")
    print("- relationships_with_types.parquet: å¸¦ç±»å‹çš„å…³ç³»æ•°æ®")
    print("- communities.parquet: ç¤¾åŒºæ•°æ®ï¼ˆç”¨äºèŠ‚ç‚¹åˆ†ç»„ï¼‰")


if __name__ == "__main__":
    main()
