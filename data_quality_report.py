#!/usr/bin/env python3
"""
GraphRAGå…³ç³»ç±»å‹æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š
"""

import pandas as pd
import re
from collections import Counter

def extract_relationship_type(description):
    """ä»æè¿°ä¸­æå–å…³ç³»ç±»å‹"""
    # é¢„å®šä¹‰çš„å…³ç³»ç±»å‹åˆ—è¡¨
    relation_types = [
        'BELONGS_TO', 'IS_A', 'RELATED_TO', 'TEACHES', 'PREREQUISITE_OF',
        'PART_OF', 'CAUSES', 'LOCATED_IN', 'MENTIONED_IN', 'WORKS_FOR',
        'MANAGES', 'CONTAINS', 'OCCURS_BEFORE', 'LEADS_TO', 'COLLABORATES_WITH',
        'OPPOSES', 'SIMILAR_TO', 'MENTIONS', 'CITES', 'AUTHORED_BY', 'PUBLISHED_IN',
        'DERIVED_FROM', 'HAS_TOPIC', 'USES', 'EXTENDS', 'HAS_PROPERTY'
    ]
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯å…³ç³»ç±»å‹
    if description.strip() in relation_types:
        return description.strip()
    
    # åœ¨æè¿°ä¸­æŸ¥æ‰¾å…³ç³»ç±»å‹
    for rel_type in relation_types:
        if rel_type in description:
            return rel_type
    
    return "UNKNOWN"

def generate_quality_report():
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('output/relationships.parquet')
    
    print("ğŸ“Š GraphRAGå…³ç³»ç±»å‹æ•°æ®è´¨é‡æŠ¥å‘Š")
    print("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»å…³ç³»æ•°: {len(df)}")
    print(f"  å”¯ä¸€æºèŠ‚ç‚¹: {df['source'].nunique()}")
    print(f"  å”¯ä¸€ç›®æ ‡èŠ‚ç‚¹: {df['target'].nunique()}")
    print(f"  å¹³å‡æƒé‡: {df['weight'].mean():.2f}")
    print(f"  æƒé‡èŒƒå›´: {df['weight'].min():.1f} - {df['weight'].max():.1f}")
    
    # æå–å…³ç³»ç±»å‹
    df['relationship_type'] = df['description'].apply(extract_relationship_type)
    
    # å…³ç³»ç±»å‹ç»Ÿè®¡
    type_counts = df['relationship_type'].value_counts()
    print(f"\nğŸ¯ å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    print(f"  è¯†åˆ«å‡ºçš„å…³ç³»ç±»å‹æ•°: {len(type_counts)}")
    print(f"  æˆåŠŸè¯†åˆ«ç‡: {(len(df) - type_counts.get('UNKNOWN', 0)) / len(df) * 100:.1f}%")
    
    print(f"\nğŸ“‹ è¯¦ç»†å…³ç³»ç±»å‹ç»Ÿè®¡:")
    for rel_type, count in type_counts.items():
        percentage = count / len(df) * 100
        print(f"  {rel_type:20s}: {count:2d} ({percentage:5.1f}%)")
    
    # è´¨é‡è¯„ä¼°
    print(f"\nâœ… è´¨é‡è¯„ä¼°:")
    
    # 1. å…³ç³»ç±»å‹è¦†ç›–åº¦
    predefined_types = [
        'BELONGS_TO', 'IS_A', 'RELATED_TO', 'TEACHES', 'PREREQUISITE_OF',
        'PART_OF', 'CAUSES', 'LOCATED_IN', 'MENTIONED_IN', 'WORKS_FOR',
        'MANAGES', 'CONTAINS', 'OCCURS_BEFORE', 'LEADS_TO', 'COLLABORATES_WITH',
        'OPPOSES', 'SIMILAR_TO', 'MENTIONS', 'CITES', 'AUTHORED_BY', 'PUBLISHED_IN',
        'DERIVED_FROM', 'HAS_TOPIC', 'USES', 'EXTENDS', 'HAS_PROPERTY'
    ]
    
    found_types = set(type_counts.keys()) - {'UNKNOWN'}
    coverage = len(found_types) / len(predefined_types) * 100
    print(f"  å…³ç³»ç±»å‹è¦†ç›–åº¦: {len(found_types)}/{len(predefined_types)} ({coverage:.1f}%)")
    
    # 2. æ•°æ®å®Œæ•´æ€§
    missing_source = df['source'].isna().sum()
    missing_target = df['target'].isna().sum()
    missing_desc = df['description'].isna().sum()
    print(f"  æ•°æ®å®Œæ•´æ€§: æºèŠ‚ç‚¹ç¼ºå¤±{missing_source}ä¸ª, ç›®æ ‡èŠ‚ç‚¹ç¼ºå¤±{missing_target}ä¸ª, æè¿°ç¼ºå¤±{missing_desc}ä¸ª")
    
    # 3. å…³ç³»æ–¹å‘æ€§
    bidirectional_count = 0
    for _, row in df.iterrows():
        reverse_exists = ((df['source'] == row['target']) & (df['target'] == row['source'])).any()
        if reverse_exists:
            bidirectional_count += 1
    print(f"  åŒå‘å…³ç³»æ•°: {bidirectional_count // 2} å¯¹")
    
    # 4. èŠ‚ç‚¹è¿æ¥åº¦åˆ†æ
    source_counts = df['source'].value_counts()
    target_counts = df['target'].value_counts()
    
    print(f"\nğŸ”— èŠ‚ç‚¹è¿æ¥åº¦åˆ†æ:")
    print(f"  æœ€æ´»è·ƒæºèŠ‚ç‚¹: {source_counts.index[0]} ({source_counts.iloc[0]} ä¸ªå‡ºåº¦)")
    print(f"  æœ€æ´»è·ƒç›®æ ‡èŠ‚ç‚¹: {target_counts.index[0]} ({target_counts.iloc[0]} ä¸ªå…¥åº¦)")
    
    # 5. å…³ç³»ç±»å‹è¯­ä¹‰åˆ†æ
    print(f"\nğŸ§  å…³ç³»ç±»å‹è¯­ä¹‰åˆ†æ:")
    
    # æŒ‰è¯­ä¹‰åˆ†ç»„
    semantic_groups = {
        'åˆ†ç±»å…³ç³»': ['IS_A', 'BELONGS_TO', 'PART_OF'],
        'ä½¿ç”¨å…³ç³»': ['USES', 'CONTAINS'],
        'å±æ€§å…³ç³»': ['HAS_PROPERTY'],
        'æ—¶é—´å…³ç³»': ['OCCURS_BEFORE', 'LEADS_TO'],
        'å¯¹ç«‹å…³ç³»': ['OPPOSES'],
        'åä½œå…³ç³»': ['COLLABORATES_WITH'],
        'ç›¸ä¼¼å…³ç³»': ['SIMILAR_TO'],
        'å¼•ç”¨å…³ç³»': ['MENTIONS'],
        'é€šç”¨å…³ç³»': ['RELATED_TO']
    }
    
    for group_name, types in semantic_groups.items():
        group_count = sum(type_counts.get(t, 0) for t in types)
        if group_count > 0:
            print(f"  {group_name}: {group_count} ä¸ªå…³ç³»")
            for t in types:
                if t in type_counts:
                    print(f"    - {t}: {type_counts[t]}")
    
    # 6. ç”Ÿæˆæ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    
    unknown_count = type_counts.get('UNKNOWN', 0)
    if unknown_count > 0:
        print(f"  - æœ‰ {unknown_count} ä¸ªå…³ç³»æœªèƒ½è¯†åˆ«ç±»å‹ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯")
    
    if coverage < 50:
        print(f"  - å…³ç³»ç±»å‹è¦†ç›–åº¦è¾ƒä½({coverage:.1f}%)ï¼Œå»ºè®®å¢åŠ æ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®")
    
    if len(found_types) < 10:
        print(f"  - å…³ç³»ç±»å‹å¤šæ ·æ€§ä¸è¶³ï¼Œå»ºè®®è°ƒæ•´å®ä½“ç±»å‹å’Œæ–‡æ¡£å†…å®¹")
    
    # ä¿å­˜å¢å¼ºçš„æ•°æ®
    df_enhanced = df.copy()
    df_enhanced['extracted_relationship_type'] = df['relationship_type']
    df_enhanced.to_parquet('output/relationships_with_types.parquet', index=False)
    print(f"\nğŸ’¾ å·²ä¿å­˜å¢å¼ºæ•°æ®åˆ°: output/relationships_with_types.parquet")
    
    return df_enhanced

if __name__ == "__main__":
    enhanced_df = generate_quality_report()
